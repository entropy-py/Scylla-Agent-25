{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing Dependencies**"
      ],
      "metadata": {
        "id": "AdXuvbqg9bPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the essential libraries from langchain and google.\n",
        "!pip install langchain langchain-google-genai chromadb requests -q\n",
        "\n",
        "print(\"Libraries installed successfully.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed successfully.\n"
          ]
        }
      ],
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEWDo_bf2oS2",
        "outputId": "828b1d4e-398c-4e73-af1f-1b7dc754233d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Configure API Key**"
      ],
      "metadata": {
        "id": "xgP_Ce3d2oS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
        "print(\"Google API Key has been configured.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google API Key has been configured.\n"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZaTayrt2oS7",
        "outputId": "e5bb6a69-5786-4d30-ccf5-5c90d649e008"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Load Data Source**"
      ],
      "metadata": {
        "id": "jemMTJHd2oS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from langchain.schema import Document\n",
        "\n",
        "# URL of the text document we want the RAG system to know about.\n",
        "url = 'https://langchain-ai.github.io/langgraph/llms.txt'\n",
        "response = requests.get(url)\n",
        "text_content = response.text\n",
        "\n",
        "# We wrap the text in a LangChain `Document` object for compatibility.\n",
        "docs = [Document(page_content=text_content)]\n",
        "\n",
        "print(\"Document loaded successfully.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document loaded successfully.\n"
          ]
        }
      ],
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8x9h3FV2oS_",
        "outputId": "c519775b-0ec8-44e0-cbe3-0a0f1fbba06d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Split Document into Chunks**\n",
        "LLMs have a limited context window, so we split the large document into smaller, overlapping chunks. This allows the model to process relevant information efficiently."
      ],
      "metadata": {
        "id": "FF3gZuED2oTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\" Document was split into {len(chunks)} chunks.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Document was split into 103 chunks.\n"
          ]
        }
      ],
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijchAJuK2oTC",
        "outputId": "5db5a0a4-b09d-4b54-9cd6-c76b161fe7eb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Create Embeddings and Vector Store**\n",
        "*We convert our text chunks into numerical vectors (embeddings) and store them in a searchable Chroma vector database.*"
      ],
      "metadata": {
        "id": "yrSkdEoz2oTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WkPwtr53roy",
        "outputId": "23392c0a-3618-4f51-f904-e4383341c8fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m174.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain_community\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Initialize the embedding model we'll use.\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings)\n",
        "print(\"Vector store created.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store created.\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcePyY922oTH",
        "outputId": "b07e0a40-5217-468f-a19f-ef2ceadb2444"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Creating the Complete RAG Chain**"
      ],
      "metadata": {
        "id": "S6_V0EAh2oTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Define the component that retrieves relevant chunks from the vector store.\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5}) # Retrieve top 5 chunks\n",
        "\n",
        "# Define the LLM we'll use to generate the answer.\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.05)\n",
        "\n",
        "# Define the prompt template to structure the information for the LLM.\n",
        "template = \"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Helper function to combine our retrieved documents into a single string.\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Build the final chain by piping all the components together.\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"RAG chain created successfully.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG chain created successfully.\n"
          ]
        }
      ],
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvZP1QHq2oTL",
        "outputId": "31aba7d6-dd91-425f-8276-ef3da5a4f970"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Ask a Question**"
      ],
      "metadata": {
        "id": "4OIXNd_F2oTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "query = \"What is the main difference between LangGraph and LangChain?\"\n",
        "response = rag_chain.invoke(query)\n",
        "\n",
        "# Print the results in a nicely formatted way.\n",
        "print(\"--- Question ---\")\n",
        "print(query)\n",
        "print(\"\\n--- Answer ---\")\n",
        "print(textwrap.fill(response, width=80))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Question ---\n",
            "What is the main difference between LangGraph and LangChain?\n",
            "\n",
            "--- Answer ---\n",
            "Based on the provided text, the FAQ for LangGraph mentions that it covers the\n",
            "differences between LangGraph and LangChain, but the specific differences are\n",
            "not detailed in these snippets.  Therefore, I don't know the answer.\n"
          ]
        }
      ],
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpXAUMfC2oTO",
        "outputId": "70e6ee62-7dfb-4297-9b92-fca54a813b29"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}