{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maP9fwYWju-c"
      },
      "source": [
        "### Overview:\n",
        "This code implements one of the multiple ways of multi-model RAG. It extracts and processes text and images from PDFs, utilizing a multi-modal Retrieval-Augmented Generation (RAG) system for summarizing and retrieving content for question answering.\n",
        "\n",
        "### Key Components:\n",
        "   - **PyMuPDF**: For extracting text and images from PDFs.\n",
        "   - **Gemini 1.5-flash model**: To summarize images and tables.\n",
        "   - **Cohere Embeddings**: For embedding document splits.\n",
        "   - **Chroma Vectorstore**: To store and retrieve document embeddings.\n",
        "   - **LangChain**: To orchestrate the retrieval and generation pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkoFc_t9ju-d"
      },
      "source": [
        "### Diagram:\n",
        "   <img src=\"https://github.com/NirDiamant/RAG_Techniques/blob/main/images/multi_model_rag_with_captioning.svg?raw=1\" alt=\"Reliable-RAG\" width=\"300\">\n",
        "\n",
        "### Motivation:\n",
        "Efficiently summarize complex documents to facilitate easy retrieval and concise responses for multi-modal data.\n",
        "\n",
        "### Method Details:\n",
        "   - Text and images are extracted from the PDF using PyMuPDF.\n",
        "   - Summarization is performed on extracted images and tables using Gemini.\n",
        "   - Embeddings are generated via Cohere for storage in Chroma.\n",
        "   - A similarity-based retriever fetches relevant sections based on the query.\n",
        "\n",
        "### Benefits:\n",
        "   - Simplified retrieval from complex, multi-modal documents.\n",
        "   - Streamlined Q&A process for both text and images.\n",
        "   - Flexible architecture for expanding to more document types.\n",
        "\n",
        "### Implementation:\n",
        "   - Documents are split into chunks with overlap using a text splitter.\n",
        "   - Summarized text and image content are stored as vectors.\n",
        "   - Queries are handled by retrieving relevant document segments and generating concise answers.\n",
        "\n",
        "### Summary:\n",
        "The project enables multi-modal document processing and retrieval, providing concise, relevant responses by combining state-of-the-art LLMs and vector-based retrieval systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhiw8tUWju-e"
      },
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_vByCF5Qju-e",
        "outputId": "9c351f27-35a0-42da-b35a-df8d16c3db23"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langchain langchain-community pillow pymupdf python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "88q073RYkoiP",
        "outputId": "06092072-18fd-4473-ad4e-944a3903dbbf"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB4IbEk6ju-f",
        "outputId": "523c73dc-c545-4d9f-a62f-66b00cce8861"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "from PIL import Image\n",
        "import io\n",
        "import os\n",
        "#from dotenv import load_dotenv\n",
        "\n",
        "import google.generativeai as genai\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_cohere import ChatCohere, CohereEmbeddings\n",
        "\n",
        "#load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0pwRNPMTmDcv"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['COHERE_API_KEY'] = userdata.get('COHERE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywmIC6zFju-g"
      },
      "source": [
        "### Download the \"Attention is all you need\" paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWHNhFz3ju-g",
        "outputId": "99f39237-fabd-427c-bf9a-901b7ef453ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-06-17 11:12:45--  https://arxiv.org/pdf/1706.03762\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.3.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2215244 (2.1M) [application/pdf]\n",
            "Saving to: ‘1706.03762’\n",
            "\n",
            "1706.03762          100%[===================>]   2.11M  6.27MB/s    in 0.3s    \n",
            "\n",
            "2025-06-17 11:12:45 (6.27 MB/s) - ‘1706.03762’ saved [2215244/2215244]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://arxiv.org/pdf/1706.03762\n",
        "!mv 1706.03762 attention_is_all_you_need.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMns0kaPju-h"
      },
      "source": [
        "### Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xkMTVjJgju-h"
      },
      "outputs": [],
      "source": [
        "text_data = []\n",
        "img_data = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kMDL5I5pju-i"
      },
      "outputs": [],
      "source": [
        "with fitz.open('attention_is_all_you_need.pdf') as pdf_file:\n",
        "    # Create a directory to store the images\n",
        "    if not os.path.exists(\"extracted_images\"):\n",
        "        os.makedirs(\"extracted_images\")\n",
        "\n",
        "    # Loop through every page in the PDF\n",
        "    for page_number in range(len(pdf_file)):\n",
        "        page = pdf_file[page_number]\n",
        "\n",
        "        # Get the text on page\n",
        "        text = page.get_text().strip()\n",
        "        text_data.append({\"response\": text, \"name\": page_number+1})\n",
        "        # Get the list of images on the page\n",
        "        images = page.get_images(full=True)\n",
        "\n",
        "        # Loop through all images found on the page\n",
        "        for image_index, img in enumerate(images, start=0):\n",
        "            xref = img[0]  # Get the XREF of the image\n",
        "            base_image = pdf_file.extract_image(xref)  # Extract the image\n",
        "            image_bytes = base_image[\"image\"]  # Get the image bytes\n",
        "            image_ext = base_image[\"ext\"]  # Get the image extension\n",
        "\n",
        "            # Load the image using PIL and save it\n",
        "            image = Image.open(io.BytesIO(image_bytes))\n",
        "            image.save(f\"extracted_images/image_{page_number+1}_{image_index+1}.{image_ext}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aMCi8rnrju-i"
      },
      "outputs": [],
      "source": [
        "genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYYoGZqiju-i"
      },
      "source": [
        "### Image Captioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "R3s0dR2kju-j"
      },
      "outputs": [],
      "source": [
        "for img in os.listdir(\"extracted_images\"):\n",
        "    image = Image.open(f\"extracted_images/{img}\")\n",
        "    response = model.generate_content([image, \"You are an assistant tasked with summarizing tables, images and text for retrieval. \\\n",
        "    These summaries will be embedded and used to retrieve the raw text or table elements \\\n",
        "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text or image:\"])\n",
        "    img_data.append({\"response\": response.text, \"name\": img})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQa-6s6yju-j"
      },
      "source": [
        "### Vectostore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "r4Y4EfL_ju-j"
      },
      "outputs": [],
      "source": [
        "# Set embeddings\n",
        "embedding_model = CohereEmbeddings(model=\"embed-english-v3.0\")\n",
        "\n",
        "# Load the document\n",
        "docs_list = [Document(page_content=text['response'], metadata={\"name\": text['name']}) for text in text_data]\n",
        "img_list = [Document(page_content=img['response'], metadata={\"name\": img['name']}) for img in img_data]\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=400, chunk_overlap=50\n",
        ")\n",
        "\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "img_splits = text_splitter.split_documents(img_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xvdPJVwomh6S",
        "outputId": "eefebd94-7c40-4743-9f3c-210727da5b21"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VFa2JP5bju-k"
      },
      "outputs": [],
      "source": [
        "# Add to vectorstore\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits + img_splits, # adding the both text and image splits\n",
        "    collection_name=\"multi_model_rag\",\n",
        "    embedding=embedding_model,\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever(\n",
        "                search_type=\"similarity\",\n",
        "                search_kwargs={'k': 1}, # number of documents to retrieve\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DlxfVh8ju-k"
      },
      "source": [
        "### Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ho_dXbQWnKhV"
      },
      "outputs": [],
      "source": [
        "queries=[\"What is the main contribution of this paper?\",\n",
        "        \"How does the Transformer model differ from recurrent and convolutional neural networks?\",\n",
        "        \"What datasets were used for training and evaluating the Transformer model?\",\n",
        "        \"What are the advantages of using self-attention?\",\n",
        "        \"Can you explain Figure 1?\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEXt0URyju-k",
        "outputId": "63b67183-175d-4c21-a6e1-8a05f354d6a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the main contribution of this paper?\n",
            "\n",
            "Relevant documents:[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
            "corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n",
            "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\n",
            "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,\n",
            "pages 152–159. ACL, June 2006.\n",
            "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
            "model. In Empirical Methods in Natural Language Processing, 2016.\n",
            "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
            "summarization. arXiv preprint arXiv:1705.04304, 2017.\n",
            "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\n",
            "and interpretable tree annotation. In Proceedings of the 21st International Conference on\n",
            "Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\n",
            "2006.\n",
            "[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\n",
            "preprint arXiv:1608.05859, 2016.\n",
            "[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\n",
            "with subword units. arXiv preprint arXiv:1508.07909, 2015.\n",
            "\n",
            "\n",
            "How does the Transformer model differ from recurrent and convolutional neural networks?\n",
            "\n",
            "Relevant documents:1\n",
            "Introduction\n",
            "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
            "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
            "transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\n",
            "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
            "architectures [38, 24, 15].\n",
            "Recurrent models typically factor computation along the symbol positions of the input and output\n",
            "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
            "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
            "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
            "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
            "significant improvements in computational efficiency through factorization tricks [21] and conditional\n",
            "computation [32], while also improving model performance in case of the latter. The fundamental\n",
            "constraint of sequential computation, however, remains.\n",
            "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
            "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
            "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
            "are used in conjunction with a recurrent network.\n",
            "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
            "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
            "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
            "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
            "2\n",
            "Background\n",
            "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
            "\n",
            "\n",
            "What datasets were used for training and evaluating the Transformer model?\n",
            "\n",
            "Relevant documents:Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\n",
            "prisingly well, yielding better results than all previously reported models with the exception of the\n",
            "Recurrent Neural Network Grammar [8].\n",
            "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\n",
            "Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
            "7\n",
            "Conclusion\n",
            "In this work, we presented the Transformer, the first sequence transduction model based entirely on\n",
            "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
            "multi-headed self-attention.\n",
            "For translation tasks, the Transformer can be trained significantly faster than architectures based\n",
            "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
            "English-to-French translation tasks, we achieve a new state of the art. In the former task our best\n",
            "model outperforms even all previously reported ensembles.\n",
            "We are excited about the future of attention-based models and plan to apply them to other tasks. We\n",
            "plan to extend the Transformer to problems involving input and output modalities other than text and\n",
            "to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\n",
            "such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
            "The code we used to train and evaluate our models is available at https://github.com/\n",
            "tensorflow/tensor2tensor.\n",
            "Acknowledgements\n",
            "We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
            "comments, corrections and inspiration.\n",
            "References\n",
            "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n",
            "arXiv:1607.06450, 2016.\n",
            "\n",
            "\n",
            "What are the advantages of using self-attention?\n",
            "\n",
            "Relevant documents:length n is smaller than the representation dimensionality d, which is most often the case with\n",
            "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
            "[38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
            "very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
            "the input sequence centered around the respective output position. This would increase the maximum\n",
            "path length to O(n/r). We plan to investigate this approach further in future work.\n",
            "A single convolutional layer with kernel width k < n does not connect all pairs of input and output\n",
            "positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\n",
            "or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\n",
            "between any two positions in the network. Convolutional layers are generally more expensive than\n",
            "recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\n",
            "considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\n",
            "convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\n",
            "the approach we take in our model.\n",
            "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\n",
            "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
            "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
            "and semantic structure of the sentences.\n",
            "5\n",
            "Training\n",
            "This section describes the training regime for our models.\n",
            "5.1\n",
            "Training Data and Batching\n",
            "\n",
            "\n",
            "Can you explain Figure 1?\n",
            "\n",
            "Relevant documents:PE(pos,2i+1) = cos(pos/100002i/dmodel)\n",
            "where pos is the position and i is the dimension. That is, each dimension of the positional encoding\n",
            "corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\n",
            "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
            "relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\n",
            "PEpos.\n",
            "We also experimented with using learned positional embeddings [9] instead, and found that the two\n",
            "versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
            "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
            "during training.\n",
            "4\n",
            "Why Self-Attention\n",
            "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
            "tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
            "(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\n",
            "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
            "consider three desiderata.\n",
            "One is the total computational complexity per layer. Another is the amount of computation that can\n",
            "be parallelized, as measured by the minimum number of sequential operations required.\n",
            "The third is the path length between long-range dependencies in the network. Learning long-range\n",
            "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
            "ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for query in queries:\n",
        "    print(query + \"\\n\")\n",
        "    docs = retriever.invoke(query)\n",
        "    print(\"Relevant documents:\" + docs[0].page_content + \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OYQYHLhsju-l"
      },
      "outputs": [],
      "source": [
        "docs = retriever.invoke(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBTR4B_pju-l"
      },
      "source": [
        "### Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8lNJzXiju-l",
        "outputId": "5c912515-2ae1-404a-f153-d62a32c1720e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query:\n",
            "What is the main contribution of this paper?\n",
            "\n",
            "Answer: \n",
            "The main contribution of this paper is the introduction of a new method for encoding positional information in neural sequence transduction models, using a sinusoidal function to generate positional encodings.\n",
            "\n",
            "\n",
            "Query:\n",
            "How does the Transformer model differ from recurrent and convolutional neural networks?\n",
            "\n",
            "Answer: \n",
            "The Transformer model differs from recurrent and convolutional neural networks in its ability to process sequential data in parallel, allowing for more efficient training on long sequences. It also uses self-attention mechanisms to capture long-range dependencies, which are challenging for recurrent and convolutional networks.\n",
            "\n",
            "\n",
            "Query:\n",
            "What datasets were used for training and evaluating the Transformer model?\n",
            "\n",
            "Answer: \n",
            "The question cannot be answered based on the provided text.\n",
            "\n",
            "\n",
            "Query:\n",
            "What are the advantages of using self-attention?\n",
            "\n",
            "Answer: \n",
            "Self-attention allows for learning long-range dependencies, which is a key challenge in sequence transduction tasks. It also offers a parallelizable way to process variable-length sequences, with a minimum of sequential operations.\n",
            "\n",
            "\n",
            "Query:\n",
            "Can you explain Figure 1?\n",
            "\n",
            "Answer: \n",
            "Figure 1 illustrates the model's positional encoding, showing how each dimension of the encoding corresponds to a sinusoid with wavelengths forming a geometric progression. The figure visualizes the hypothesis that this encoding allows the model to learn relative positions, as positional encodings for nearby positions (e.g., pos and pos+k) can be represented as linear functions of each other.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge.\n",
        "Use three-to-five sentences maximum and keep the answer concise.\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved documents: \\n\\n <docs>{documents}</docs> \\n\\n User question: <question>{question}</question>\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# LLM\n",
        "llm = ChatCohere(model=\"command-r-plus\", temperature=0)\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "for query in queries:\n",
        "  print(\"Query:\\n\"+ query + \"\\n\")\n",
        "  generation = rag_chain.invoke({\"documents\":docs[0].page_content, \"question\": query})\n",
        "  print(\"Answer: \\n\"+ generation + \"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
