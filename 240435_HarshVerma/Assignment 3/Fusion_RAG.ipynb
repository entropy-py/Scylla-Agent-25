{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Fusion RAG**"
      ],
      "metadata": {
        "id": "lB0rAAqOieQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Installation**"
      ],
      "metadata": {
        "id": "UbCzWMMfiYvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-openai langchain-community faiss-cpu sentence-transformers langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyBDNymBZ7xU",
        "outputId": "9333ddfb-b6a3-4485-ecb6-128b9ff080a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# --- SETUP ---\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJTLxXKdaO64",
        "outputId": "e79f7806-6f73-4fc6-88de-0916b1e7d100"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. CREATE A SAMPLE DOCUMENT STORE**"
      ],
      "metadata": {
        "id": "nA9nqcYninTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    Document(page_content=\"RAG-Fusion combines generative AI with a fusion-based retrieval process to improve answer quality.\"),\n",
        "    Document(page_content=\"Reciprocal Rank Fusion (RRF) is an algorithm used to combine multiple ranked lists into a single, more robust list.\"),\n",
        "    Document(page_content=\"Multi-query retrieval involves generating several variations of a user's query to broaden the search scope.\"),\n",
        "    Document(page_content=\"For complex questions, breaking them down into sub-queries can yield more accurate results from a vector database.\"),\n",
        "    Document(page_content=\"Vector search finds documents based on semantic similarity rather than exact keyword matches.\"),\n",
        "    Document(page_content=\"The final step in RAG is generation, where an LLM synthesizes an answer from the retrieved context.\"),\n",
        "    Document(page_content=\"Advanced RAG techniques often involve query transformations to better match the user's intent with the stored data.\"),\n",
        "]"
      ],
      "metadata": {
        "id": "vLJNqmJsaMWT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. USE A FREE, LOCAL EMBEDDING MODEL**"
      ],
      "metadata": {
        "id": "zNYRSDh9irEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "print(\"Loading local HuggingFace embedding model...\")\n",
        "model_name = \"all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
        "print(\"Embedding model loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVGfDGwWaJH6",
        "outputId": "31c9cbb0-3290-4644-9ca0-855c312840c4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading local HuggingFace embedding model...\n",
            "Embedding model loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. CREATE THE VECTOR STORE (FREE)**"
      ],
      "metadata": {
        "id": "5D9GC42Zi2-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating vector store using local embeddings...\")\n",
        "vector_store = FAISS.from_documents(documents, embeddings)\n",
        "retriever = vector_store.as_retriever()\n",
        "print(\"Vector store created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYJfkcYfaHgQ",
        "outputId": "ab5d8a12-3aa2-44e7-f445-5e5306e3c826"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating vector store using local embeddings...\n",
            "Vector store created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. RAG-FUSION LOGIC WITH A FREE GENERATION MODEL**"
      ],
      "metadata": {
        "id": "lf2ydbeDjCkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_openai import ChatOpenAI\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Use Groq's Llama 3 model for all text generation tasks (query generation and final answer)\n",
        "llm = ChatGroq(temperature=0, model_name=\"llama3-8b-8192\")\n",
        "\n",
        "query_gen_template = \"\"\"\n",
        "You are a helpful assistant that generates multiple search queries based on a single input query.\n",
        "Generate {num_queries} search queries, one on each line, related to this input: {original_query}\n",
        "\"\"\"\n",
        "query_gen_prompt = ChatPromptTemplate.from_template(query_gen_template)\n",
        "query_generator = (\n",
        "    query_gen_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")\n",
        "\n",
        "def reciprocal_rank_fusion(retrieved_lists, k=60):\n",
        "    fused_scores = {}\n",
        "    for doc_list in retrieved_lists:\n",
        "        for rank, doc in enumerate(doc_list):\n",
        "            doc_str = doc.page_content\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            fused_scores[doc_str] += 1 / (k + rank)\n",
        "    reranked_results = [\n",
        "        (Document(page_content=doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "    return reranked_results\n",
        "\n",
        "def rag_fusion_retrieval(original_query, retriever, num_queries=4):\n",
        "    print(f\"\\nOriginal Query: {original_query}\")\n",
        "    generated_queries = query_generator.invoke({\"original_query\": original_query, \"num_queries\": num_queries})\n",
        "    print(f\"Generated Queries: {generated_queries}\")\n",
        "    all_retrieved_docs = [retriever.get_relevant_documents(q) for q in generated_queries]\n",
        "    final_ranked_docs = reciprocal_rank_fusion(all_retrieved_docs)\n",
        "    return final_ranked_docs"
      ],
      "metadata": {
        "id": "lnICchiTaEo8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. EXECUTION AND FINAL ANSWER GENERATION (NOW FULLY FREE)**"
      ],
      "metadata": {
        "id": "NuWz2RuAjH_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_query = \"how does RAG-Fusion work?\"\n",
        "fused_documents_with_scores = rag_fusion_retrieval(original_query, retriever)\n",
        "\n",
        "print(\"\\n--- Top Fused Documents ---\")\n",
        "for doc, score in fused_documents_with_scores[:4]:\n",
        "    print(f\"Score: {score:.4f}\\tContent: {doc.page_content}\")\n",
        "\n",
        "final_context = \"\\n\".join([doc.page_content for doc, _ in fused_documents_with_scores[:4]])\n",
        "generation_prompt_template = \"\"\"\n",
        "You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "generation_prompt = ChatPromptTemplate.from_template(generation_prompt_template)\n",
        "final_chain = generation_prompt | llm | StrOutputParser()\n",
        "\n",
        "final_answer = final_chain.invoke({\n",
        "    \"context\": final_context,\n",
        "    \"question\": original_query\n",
        "})\n",
        "\n",
        "print(\"\\n--- Final Generated Answer ---\")\n",
        "print(final_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2Qh-ZGyaBxX",
        "outputId": "422c9a3b-d1a2-4284-9a45-2d8708cf09f0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original Query: how does RAG-Fusion work?\n",
            "Generated Queries: ['Here are 4 search queries related to \"how does RAG-Fusion work?\":', '', '1. \"RAG-Fusion mechanism of action\"', '2. \"How does RAG-Fusion therapy work for cancer treatment\"', '3. \"RAG-Fusion gene editing technique explanation\"', '4. \"RAG-Fusion protein fusion mechanism and applications\"', '', 'Let me know if you need anything else!']\n",
            "\n",
            "--- Top Fused Documents ---\n",
            "Score: 0.1304\tContent: RAG-Fusion combines generative AI with a fusion-based retrieval process to improve answer quality.\n",
            "Score: 0.1158\tContent: The final step in RAG is generation, where an LLM synthesizes an answer from the retrieved context.\n",
            "Score: 0.0973\tContent: Advanced RAG techniques often involve query transformations to better match the user's intent with the stored data.\n",
            "Score: 0.0799\tContent: Reciprocal Rank Fusion (RRF) is an algorithm used to combine multiple ranked lists into a single, more robust list.\n",
            "\n",
            "--- Final Generated Answer ---\n",
            "Based on the provided context, RAG-Fusion works by combining generative AI with a fusion-based retrieval process to improve answer quality. The process involves retrieving relevant context and then using a Large Language Model (LLM) to synthesize an answer from the retrieved context.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}