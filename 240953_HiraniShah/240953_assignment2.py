# -*- coding: utf-8 -*-
"""240953_Assignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fPIIKGMlB0e8_so7Yqzh1-ul8jESVXcZ
"""

import os
from dotenv import load_dotenv
load_dotenv()

from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import CohereEmbeddings
from langchain.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate
from langchain.llms import Cohere
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser


COHERE_API_KEY = os.getenv("COHERE_API_KEY")


loader = TextLoader("rag_context.txt")
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = text_splitter.split_documents(documents)


embedding = CohereEmbeddings(model="embed-english-light-v3.0", cohere_api_key=COHERE_API_KEY)
vectorstore = FAISS.from_documents(chunks, embedding)

retriever = vectorstore.as_retriever()


template = """You are a helpful assistant for question answering.
Use the following context to answer the question.
If you don't know the answer, say "I don't know".

Context: {context}
Question: {question}

Answer (max 3 sentences):
"""
prompt = ChatPromptTemplate.from_template(template)


llm = Cohere(model="command-light", temperature=0, cohere_api_key=COHERE_API_KEY)

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)


while True:
    query = input("\nAsk a question (or type 'exit'): ")
    if query.lower() == "exit":
        break
    print("\nAnswer:\n", rag_chain.invoke(query))