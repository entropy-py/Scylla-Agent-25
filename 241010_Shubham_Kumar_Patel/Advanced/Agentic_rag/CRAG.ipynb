{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CRAG: Corrective RAG process**"
      ],
      "metadata": {
        "id": "qDrLQWvKWHn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Neccesary libraries"
      ],
      "metadata": {
        "id": "i5Ypqib1WT7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH_wud0_bSKN",
        "outputId": "7b5b5a7c-7db8-4d6d-ce9c-ef2acab7868f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaF8GREXa_I-",
        "outputId": "fc308ba5-99fd-42e2-aa84-5a1dbf0d0c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/304.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m297.0/304.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2SAc-AjV-p1",
        "outputId": "fb7d7f4e-79c9-4c2f-b96b-6c0c09ccb263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q python-dotenv langchain-cohere langchain-community langchain-groq langchain-tavily"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Neccesary libraries"
      ],
      "metadata": {
        "id": "PO4M6RHjWgZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_cohere import CohereEmbeddings\n",
        "from langchain_community.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import PromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.chains import LLMChain\n",
        "from typing import List\n",
        "from langchain.prompts import PromptTemplate\n",
        "import json\n",
        "from typing import List, Tuple\n",
        "import time\n",
        "from langchain_tavily import TavilySearch\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "r60QHi2AWo0A"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding the documents\n"
      ],
      "metadata": {
        "id": "ZprweFYiWsgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "0LTKYo0AZgnx",
        "outputId": "6b26b72e-dab3-4118-aa16-709e7a6c7914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8dab66ab-8885-4b80-9c5e-5ab80772ab2e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8dab66ab-8885-4b80-9c5e-5ab80772ab2e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"data.zip\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"data\")\n",
        "\n",
        "\n",
        "os.listdir(\"data\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEtxY3j0aKOF",
        "outputId": "cb32eb00-0460-497b-c58f-95a389c581ee"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(\"data/data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0qXKoKAa3JT",
        "outputId": "d1ccb0f8-037f-4ab2-c688-d356003640b0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['final_draft.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
        "\n",
        "\n",
        "def get_cohere_embedder():\n",
        "    return CohereEmbeddings(\n",
        "        cohere_api_key=COHERE_API_KEY,\n",
        "        model=\"embed-english-v3.0\"\n",
        "    )"
      ],
      "metadata": {
        "id": "-45tUlpwW0Jq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexing the documents"
      ],
      "metadata": {
        "id": "2MF9q7ECW53C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents_from_dir(data_dir=\"data/\"):\n",
        "    loaders = [\n",
        "        TextLoader,     # .txt\n",
        "        PyPDFLoader,    # .pdf\n",
        "    ]\n",
        "\n",
        "    docs = []\n",
        "    for file in os.listdir(data_dir):\n",
        "        file_path = os.path.join(data_dir, file)\n",
        "        ext = file.lower().split(\".\")[-1]\n",
        "\n",
        "        if ext == \"txt\":\n",
        "            docs.extend(TextLoader(file_path).load())\n",
        "        elif ext == \"pdf\":\n",
        "            docs.extend(PyPDFLoader(file_path).load())\n",
        "        # add more formats here if needed\n",
        "\n",
        "    return docs\n",
        "\n",
        "def create_or_load_faiss(index_path=\"faiss_index\", data_dir=\"data/\"):\n",
        "    embedder = get_cohere_embedder()\n",
        "    if os.path.exists(index_path):\n",
        "        return FAISS.load_local(\n",
        "            index_path,\n",
        "            embedder,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "    docs = load_documents_from_dir(data_dir)\n",
        "    vectorstore = FAISS.from_documents(docs, embedder)\n",
        "    vectorstore.save_local(index_path)\n",
        "    return vectorstore"
      ],
      "metadata": {
        "id": "GZJHpt8PW-i3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM"
      ],
      "metadata": {
        "id": "ApeGNyvgXNnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "\n",
        "def get_groq_llm(model=\"llama3-8b-8192\"):\n",
        "    return ChatGroq(\n",
        "        groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
        "        model_name=model,\n",
        "    )"
      ],
      "metadata": {
        "id": "megGdLYOXSBW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some useful tools"
      ],
      "metadata": {
        "id": "uxenVd5EXflN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llm = get_groq_llm()\n",
        "\n",
        "class RetrievalEvaluatorInput(BaseModel):\n",
        "    relevance_score: float = Field(..., description=\"Relevance of the document to the query, from 0 to 1.\")\n",
        "\n",
        "import re\n",
        "\n",
        "def retrieval_evaluator(query: str, document: str) -> float:\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"query\", \"document\"],\n",
        "        template=\"On a scale from 0 to 1, how relevant is this document to the query?\\nQuery: {query}\\nDocument: {document}\\nRelevance score:\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    result = chain.invoke({\"query\": query, \"document\": document}).content.strip()\n",
        "\n",
        "    # Extract first float from response\n",
        "    match = re.search(r\"\\b([01](?:\\.\\d+)?)\\b\", result)\n",
        "    if match:\n",
        "        return float(match.group(1))\n",
        "\n",
        "    print(f\"[Warning] Could not parse relevance score from LLM output: {result}\")\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "\n",
        "class KnowledgeRefinementInput(BaseModel):\n",
        "    key_points: str = Field(..., description=\"Bullet-point summary of the document.\")\n",
        "\n",
        "def knowledge_refinement(doc: str) -> List[str]:\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"document\"],\n",
        "        template=\"Extract key bullet points from this document:\\n{document}\\nBullet Points:\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    output = chain.invoke({\"document\": doc}).content\n",
        "    return [line.strip(\"-• \\n\") for line in output.splitlines() if line.strip()]\n",
        "\n"
      ],
      "metadata": {
        "id": "8VpE5kkuXiqb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Websearch Tools"
      ],
      "metadata": {
        "id": "2y1Bt9DRXtgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = get_groq_llm()\n",
        "\n",
        "search = TavilySearch(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
        "\n",
        "def parse_search_results(results_string: str) -> List[Tuple[str, str]]:\n",
        "    try:\n",
        "        results = json.loads(results_string)\n",
        "        return [(r.get(\"title\", \"Untitled\"), r.get(\"link\", \"\")) for r in results]\n",
        "    except json.JSONDecodeError:\n",
        "        return []\n",
        "\n",
        "def rewrite_query(query: str) -> str:\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"query\"],\n",
        "        template=\"Rewrite the following query to make it more suitable for a web search:\\n{query}\\nRewritten query:\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    result = chain.invoke({\"query\": query}).content.strip()\n",
        "    return result\n",
        "\n",
        "def perform_web_search(query: str):\n",
        "    print(\"Performing web search...\")\n",
        "    rewritten_query = rewrite_query(query)\n",
        "\n",
        "    results = search.invoke(rewritten_query)  # returns a dict with a \"results\" list\n",
        "\n",
        "    sources = [(r.get(\"title\", \"Untitled\"), r.get(\"url\", \"\")) for r in results.get(\"results\", [])]\n",
        "    all_content = \"\\n\\n\".join([r.get(\"content\", \"\") for r in results.get(\"results\", [])])\n",
        "    refined_knowledge = knowledge_refinement(all_content)\n",
        "\n",
        "    return refined_knowledge, sources"
      ],
      "metadata": {
        "id": "UWrHGN2AXxP-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CRAG"
      ],
      "metadata": {
        "id": "CfQ52NOzYatn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crag_pipeline(query: str):\n",
        "    vs = create_or_load_faiss(index_path=\"faiss_index\", data_dir=r\"data/data\")\n",
        "    llm = get_groq_llm()\n",
        "    docs = vs.similarity_search(query, k=3)\n",
        "\n",
        "    # Evaluate relevance\n",
        "    scores = [retrieval_evaluator(query, doc.page_content) for doc in docs]\n",
        "    max_score = max(scores)\n",
        "    print(f\"Relevance scores: {scores}\")\n",
        "\n",
        "    if max_score > 0.7:\n",
        "        knowledge = \"\\n\".join([pt for doc in docs for pt in knowledge_refinement(doc.page_content)])\n",
        "        sources = [(\"Local Document\", \"\")]\n",
        "\n",
        "    elif max_score < 0.3:\n",
        "        knowledge, sources = perform_web_search(query)\n",
        "\n",
        "    else:\n",
        "        retrieved = \"\\n\".join([doc.page_content for doc in docs])\n",
        "        web_knowledge, sources = perform_web_search(query)\n",
        "        refined = knowledge_refinement(retrieved + \"\\n\\n\" + \"\\n\".join(web_knowledge))\n",
        "        knowledge = \"\\n\".join(refined)\n",
        "\n",
        "    # Final Answer\n",
        "    response_prompt = PromptTemplate(\n",
        "        input_variables=[\"query\", \"knowledge\", \"sources\"],\n",
        "        template=\"\"\"Use the following knowledge to answer the query.\n",
        "          Query: {query}\n",
        "          Knowledge:\n",
        "          {knowledge}\n",
        "          Sources:\n",
        "          {sources}\n",
        "          Answer:\"\"\"\n",
        "    )\n",
        "\n",
        "    formatted_sources = \"\\n\".join([f\"{t}: {l}\" if l else t for t, l in sources])\n",
        "    final = response_prompt | llm\n",
        "    return final.invoke({\"query\": query, \"knowledge\": knowledge, \"sources\": formatted_sources}).content\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    while True:\n",
        "        query = input(\"Query (or type 'exit' to quit): \")\n",
        "        if query.lower() == \"exit\":\n",
        "            break\n",
        "        print(crag_pipeline(query))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx03zAOVYZQN",
        "outputId": "8f394c11-e949-49e8-dd64-582631cc3b2c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query (or type 'exit' to quit): What is ScyllaAgent?\n",
            "Relevance scores: [0.7, 0.0, 0.0]\n",
            "Performing web search...\n",
            "According to the provided knowledge, ScyllaAgent is a \"Scalable and low-latency agentic chatbot system\" that is built using modular LLM workflows and includes core modules such as NLP processing, transformer models, RAGs, LangGraph, and API-based chatbot deployment using platforms like Groq.\n",
            "Query (or type 'exit' to quit): What are APIs?\n",
            "Relevance scores: [1.0, 0.85, 0.8]\n",
            "According to the provided knowledge, an API (Application Programming Interface) is a set of protocols (rules) that enables different software components to communicate and transfer data. It consists of a request and response cycle, where an API client sends a request to an API server, which retrieves data and returns it to the user. The API request includes an endpoint, method, parameters, request headers, and request body, while the API response includes a status code, response headers, and response body.\n",
            "\n",
            "APIs can be categorized into three types: private APIs (internal use within an organization), public APIs (accessible by third-party developers), and partner APIs (used between business partners with access control). Additionally, there are different API architectural styles, such as REST (Representational State of Resource), SOAP (Simple Object Access Protocol), and gRPC.\n",
            "\n",
            "In the context of Python, the Requests library can be used to make GET requests, for example. The response object returned by the request can be accessed to retrieve information such as the status code.\n",
            "\n",
            "In the context of Large Language Models (LLMs), APIs are used to process and generate text. The LLM API architecture typically consists of a neural network, data processing layer, training infrastructure, API interface, security and privacy mechanisms, and scalability features.\n",
            "\n",
            "Groq API can be used to implement a chatbot, where the setup involves installing the required packages, storing the API key securely, and initializing the Groq client and selecting a model. The chatbot can then take user input and temperature (controls creativity) to generate responses.\n",
            "Query (or type 'exit' to quit): What is the date today?\n",
            "Relevance scores: [0.0, 0.0, 0.0]\n",
            "Performing web search...\n",
            "A clever query! Unfortunately, the provided knowledge does not contain the current date. The information is about the capabilities of a date calculator, the holidays in 2025, and various links to calendars and date calculators. There is no mention of the current date.\n",
            "Query (or type 'exit' to quit): What is today's date?\n",
            "Relevance scores: [0.0, 0.0, 0.0]\n",
            "Performing web search...\n",
            "According to the provided knowledge, today's date is 06/20/2025 (mm/dd/yyyy) or 20/06/2025 (dd/mm/yyyy).\n",
            "Query (or type 'exit' to quit): What is 67*89+90*7\n",
            "Relevance scores: [0.0, 0.0, 0.0]\n",
            "Performing web search...\n",
            "I'm happy to help! However, it seems that the query is asking a mathematical question, not extracting key bullet points from a document or text. Therefore, I'll do my best to solve the mathematical problem.\n",
            "\n",
            "To calculate the result, I'll follow the order of operations (PEMDAS):\n",
            "\n",
            "1. Multiply 67 and 89: 67*89 = 5953\n",
            "2. Multiply 90 and 7: 90*7 = 630\n",
            "3. Add the two results: 5953 + 630 = 6583\n",
            "\n",
            "So, the answer to the query is: 6583\n",
            "Query (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ub29YghhZZBN"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}