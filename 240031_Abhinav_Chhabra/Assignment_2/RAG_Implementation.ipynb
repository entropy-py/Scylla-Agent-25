{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the user"
      ],
      "metadata": {
        "id": "98jQ67K--3jV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"USER_AGENT\"] = \"my-RAG-bot/1.0\"\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "CcnZKQqx-3RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a web base loader to load the text from the wikipedia page"
      ],
      "metadata": {
        "id": "wGS0FB2G--66"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMMJ6jEf9Lgv"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from langchain.schema import Document\n",
        "url = 'https://langchain-ai.github.io/langgraph/llms.txt'\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "docs = \"\".join(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we split the document into chunks of size 500 with a chunk overlap of 50"
      ],
      "metadata": {
        "id": "3dXySP2i_CqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "texts = text_splitter.split_text(docs)\n",
        "print(f\"Number of chunks: {len(texts)}\")"
      ],
      "metadata": {
        "id": "_qzzc0RH_Hab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b9ebcc2-074b-48ba-cebd-0bb10bdd77e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we convert the text into embeddings and store them in a vector store of weaviate"
      ],
      "metadata": {
        "id": "bTjM4hih_1Jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",google_api_key=GOOGLE_API_KEY,\n",
        "                             temperature=0.1,convert_system_message_to_human=True)\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)\n",
        "print(\"The data has been embedded\")"
      ],
      "metadata": {
        "id": "9fTfY9sNAC6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6392d62f-c776-4791-8d24-7745075ea988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The data has been embedded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the retriever"
      ],
      "metadata": {
        "id": "kCDEjyNUCwX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = Chroma.from_texts(texts, embeddings).as_retriever(search_kwargs={\"k\":5})\n"
      ],
      "metadata": {
        "id": "thX0rYc1DGIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We setup the template of the prompt to be sent to the model"
      ],
      "metadata": {
        "id": "7tFPXgOGDHNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wv3HvjIDL8q",
        "outputId": "f1fd1f22-4485-4aa9-8b97-f990582d321f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse three sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"), additional_kwargs={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will create the RAG pipeline sending in the formated dictionary of the query, context and instructions and obtaining responses to queries"
      ],
      "metadata": {
        "id": "SvlYOkRkDShP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "RAG_prompt = PromptTemplate.from_template(template)\n",
        "RAG_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": RAG_prompt}\n",
        ")\n",
        "query = \"Explain parts of LLMs\"\n",
        "response = RAG_chain.invoke({'query' : query})\n",
        "print(response['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J_RJ-z6EYas",
        "outputId": "964b7329-c10b-426b-ad6e-48b42f26b876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLMs consist of components like agent architectures (including routers, tool-calling agents, and memory management)  and methods for handling streaming outputs.  The provided text focuses on documentation and usage of LLMs, not their internal structure.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://langchain-ai.github.io/langgraph/llms.txt'\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "docs = \"\".join(text)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "texts = text_splitter.split_text(docs)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",google_api_key=GOOGLE_API_KEY,\n",
        "                             temperature=0.1,convert_system_message_to_human=True)\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)\n",
        "print(\"The data has been embedded\")\n",
        "retriever = Chroma.from_texts(texts, embeddings).as_retriever(search_kwargs={\"k\":5})\n",
        "template = \"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use one sentence maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "RAG_prompt = PromptTemplate.from_template(template)\n",
        "RAG_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": RAG_prompt}\n",
        ")\n",
        "query = \"Explain parts of LLMs\"\n",
        "response = RAG_chain.invoke({'query' : query})\n",
        "print(response['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OjCe8XP_JdB",
        "outputId": "2affa2d0-2414-4683-a297-45e5d22f933e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The data has been embedded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLMs include components like routers, tool-calling agents, memory management, and planning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "print(f\"User: {query}\")\n",
        "wrapped = textwrap.wrap(response['result'],width = 30)\n",
        "print(f\"Bot:\")\n",
        "for line in wrapped:\n",
        "  print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8HIGV0VBrXa",
        "outputId": "150eacb8-04ad-490b-ffc8-039639c53a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Explain parts of LLMs\n",
            "Bot:\n",
            "LLMs include components like\n",
            "routers, tool-calling agents,\n",
            "memory management, and\n",
            "planning.\n"
          ]
        }
      ]
    }
  ]
}