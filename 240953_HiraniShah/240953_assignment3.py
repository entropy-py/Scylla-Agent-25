# -*- coding: utf-8 -*-
"""240953_Assignment3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YhCnlJ8d9zJqI2IVMxAVprh_W_PIzwNn
"""

import csv
import difflib
from google.colab import userdata
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI

GOOGLE_API_KEY = userdata.get("GOOGLE_API_KEY")


def load_knowledge_base(csv_path):
    knowledge_base = []
    with open(csv_path, "r", encoding="utf-8") as file:
        reader = csv.DictReader(file)
        for row in reader:
            knowledge_base.append(row["content"])
    return knowledge_base


def dartboard_retrieve(query, knowledge_base, top_k=3):
    scored = []
    for chunk in knowledge_base:
        score = difflib.SequenceMatcher(None, query.lower(), chunk.lower()).ratio()
        scored.append((chunk, score))
    top_chunks = sorted(scored, key=lambda x: x[1], reverse=True)[:top_k]
    return [chunk for chunk, _ in top_chunks]


def show_context(context_chunks):
    for i, chunk in enumerate(context_chunks):
        print(f"--- Context {i+1} ---")
        print(chunk, end="\n\n")


llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=GOOGLE_API_KEY)


template = """Answer the question based only on the following context:
{context}

Question: {question}
"""
prompt = PromptTemplate.from_template(template)
parser = StrOutputParser()


def run_rag(query, context_chunks):
    context = "\n".join(context_chunks)
    prompt_input = prompt.invoke({"context": context, "question": query})
    response = llm | parser
    return response.invoke(prompt_input)



kb = load_knowledge_base("data/facts.csv")

query = "What causes global warming?"


top_chunks = dartboard_retrieve(query, kb, top_k=2)

show_context(top_chunks)


final_answer = run_rag(query, top_chunks)
print("Final Answer:\n", final_answer)