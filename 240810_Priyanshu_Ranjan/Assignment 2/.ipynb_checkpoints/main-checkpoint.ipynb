{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6408c580-a681-4adc-947b-8a09658faefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aab3f39b-62de-41cb-819e-89a066722c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('./altmanted.txt')\n",
    "documents = loader.load()\n",
    "# print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5660cb85-4cf1-4ed6-9eb0-686031f60d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk count: 60\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(documents)\n",
    "print(\"Chunk count:\", len(chunks))\n",
    "# len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f976c34-d63f-4e3c-a4fe-3dce56af6bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 0 ---\n",
      "\n",
      "Chris Anderson: Sam, welcome to TED. Thank you so much for coming. Sam Altman: Thank you. It's an honor. CA: Your company has been releasing crazy insane new models\n",
      "pretty much every other week it feels like. I've been playing with a couple of them. I'd like to show you what I've been playing.\n",
      "So, Sora, this is the image and video generator. I asked Sora this:\n",
      "What will it look like when you share some shocking revelations here at TED?\n",
      "You want to see how it imagined it, you know? (Laughter)\n",
      "I mean, not bad, right? How would you grade that? Five fingers on all hands. SA: Very close to what I'm wearing, you know, it's good.\n",
      "CA: I've never seen you quite that animated. SA: No, I'm not that animated of a person. CA: So maybe a B-plus.\n",
      "\n",
      "--- Chunk 1 ---\n",
      "\n",
      "But this one genuinely astounded me. When I asked it to come up with a diagram\n",
      "that shows the difference between intelligence and consciousness. Like how would you do that?\n",
      "This is what it did. I mean, this is so simple, but it's incredible.\n",
      "What is the kind of process that would allow -- like this is clearly not just image generation. It's linking into the core intelligences that your overall model has.\n",
      "SA: Yeah, the new image generation model is part of GPT-4o, so it's got all of the intelligence in there.\n",
      "And I think that's one of the reasons it's been able to do these things that people really love. CA: I mean, if I'm a management consultant and I'm playing with some of this stuff,\n",
      "\n",
      "--- Chunk 2 ---\n",
      "\n",
      "I'm thinking, uh oh, what does my future look like? SA: I mean, I think there are sort of two views you can take.\n",
      "You can say, oh, man, it's doing everything I do. What's going to happen to me? Or you can say,\n",
      "like through every other technological revolution in history, OK, now there's this new tool.\n",
      "I can do a lot more. What am I going to be able to do? It is true that the expectation\n",
      "of what we’ll have for someone in a particular job increases, but the capabilities will increase so dramatically\n",
      "that I think it will be easy to rise to that occasion. CA: So this impressed me too. I asked it to imagine Charlie Brown as thinking of himself as an AI.\n",
      "It came up with this. I thought this was actually rather profound.\n",
      "What do you think? (Laughs)\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i} ---\\n\")\n",
    "    print(chunk.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65a76395-f6ea-49ae-a9fe-43771129c491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Chris Anderson: Sam, welcome to TED. Thank you so much for coming. Sam Altman: Thank you. It's an honor. CA: Your company has been releasing crazy insane new models\n",
      "pretty much every other week it feels like. I've been playing with a couple of them. I'd like to show you what I've been playing.\n",
      "So, Sora, this is the image and video generator. I asked Sora this:\n",
      "What will it look like when you share some shocking revelations here at TED?\n",
      "You want to see how it imagined it, you know? (Laughter)\n",
      "I mean, not bad, right? How would you grade that? Five fingers on all hands. SA: Very close to what I'm wearing, you know, it's good.\n",
      "CA: I've never seen you quite that animated. SA: No, I'm not that animated of a person. CA: So maybe a B-plus.' metadata={'source': './altmanted.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e771fc8-2a06-4cd1-aa51-cabdce6a73f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.26.7\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "print(weaviate.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e42d2a14-16a6-41e0-a390-04fad691a19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'weaviate.client.Client'>\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "print(weaviate.Client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba34949d-9fc3-4f96-b910-88cf10c013a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26073/215966560.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=model_name)\n",
      "/home/itspriiyanshu/Desktop/Scylla-Agent-25/240810_Priyanshu_Ranjan/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14b36ebc-d00b-4fd4-a468-0d91da259b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded weaviate is already listening on port 8079\n",
      "Embedded weaviate wasn't listening on port 8079, so starting embedded weaviate again\n",
      "Started /home/itspriiyanshu/.cache/weaviate-embedded: process ID 26798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Shutting down... \",\"time\":\"2025-06-18T00:53:53+05:30\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Stopped serving weaviate at http://127.0.0.1:8079\",\"time\":\"2025-06-18T00:53:53+05:30\"}\n",
      "{\"action\":\"startup\",\"default_vectorizer_module\":\"none\",\"level\":\"info\",\"msg\":\"the default vectorizer modules is set to \\\"none\\\", as a result all new schema classes without an explicit vectorizer setting, will use this vectorizer\",\"time\":\"2025-06-18T00:53:53+05:30\"}\n",
      "{\"action\":\"startup\",\"auto_schema_enabled\":true,\"level\":\"info\",\"msg\":\"auto schema enabled setting is set to \\\"true\\\"\",\"time\":\"2025-06-18T00:53:53+05:30\"}\n",
      "{\"level\":\"info\",\"msg\":\"No resource limits set, weaviate will use all available memory and CPU. To limit resources, set LIMIT_RESOURCES=true\",\"time\":\"2025-06-18T00:53:53+05:30\"}\n",
      "{\"level\":\"warning\",\"msg\":\"Multiple vector spaces are present, GraphQL Explore and REST API list objects endpoint module include params has been disabled as a result.\",\"time\":\"2025-06-18T00:53:53+05:30\"}\n",
      "{\"action\":\"grpc_startup\",\"level\":\"info\",\"msg\":\"grpc server listening at [::]:50060\",\"time\":\"2025-06-18T00:53:53+05:30\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Serving weaviate at http://127.0.0.1:8079\",\"time\":\"2025-06-18T00:53:53+05:30\"}\n",
      "{\"level\":\"info\",\"msg\":\"Created shard langchain_9dae6ca55c684dc68d8f284ded73132f_cMpqiMB12gl2 in 720.138µs\",\"time\":\"2025-06-18T00:53:53+05:30\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2025-06-18T00:53:53+05:30\",\"took\":37120}\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "from langchain.vectorstores import Weaviate\n",
    "# ✅ Embedded instance for weaviate-client==3.x\n",
    "client = weaviate.Client(\n",
    "    embedded_options=EmbeddedOptions()\n",
    ")\n",
    "\n",
    "vectorstore = Weaviate.from_documents(\n",
    "    client=client,\n",
    "    documents=chunks,\n",
    "    embedding=embedding,\n",
    "    by_text=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c1b329c-f702-40a9-b520-fe334a4a33d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe00ec15-03da-4339-83fa-bf3625eb3057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the question. \\nIf you don't know the answer, just say that you don't know. \\nSA refers to Sam Altman.\\nCA refers to Chris Anderson.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\\n\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "SA refers to Sam Altman.\n",
    "CA refers to Chris Anderson.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63d8af02-cf92-46c2-a765-5c092dd6fe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itspriiyanshu/Desktop/Scylla-Agent-25/240810_Priyanshu_Ranjan/venv/lib/python3.13/site-packages/langchain_community/chat_models/openai.py:495: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  response = response.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"According to the context, Sam Altman (SA) mentions that they are building tools that will make it easier for new people to create better art, content, and novels, and that they believe humans will be at the center of this process. He also mentions that they need to figure out a new model around the economics of creative output, as the access to creativity gets increasingly democratized.\\n\\nRegarding the question of what Sam says about the democratization of access to creativity, he believes that people have been building on the creativity of others for a long time, and that as access to creativity gets democratized, there will be incredible new business models to explore.\\n\\nHe also mentions that their image-gen tool won't produce art in the style of a living artist without consent, but will do so for a particular vibe, studio, or art movement.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "GROQ_API_KEY= os.environ['GROQ_API_KEY']\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq's OpenAI-compatible URL\n",
    "    api_key=GROQ_API_KEY,               # Replace with your key\n",
    "    model=\"llama3-8b-8192\",                    # or llama3-70b-8192, mixtral-8x7b-32768, gemma-7b-it\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    ")\n",
    "\n",
    "query = \"What sam say on democrtization of access to creativity?\"\n",
    "# retrieved = retriever.invoke(\"What did Sam Altman say about AI's threat to IP rights?\")\n",
    "# print(\"Retrieved Context:\\n\", retrieved)\n",
    "\n",
    "rag_chain.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
