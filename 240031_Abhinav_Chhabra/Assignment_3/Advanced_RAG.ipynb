{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Hypothetical Prompt Embeddings (HyPE)"
      ],
      "metadata": {
        "id": "i7-HqBHsm6OC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key components :\n",
        "\n",
        "*   PDF processing and text extraction\n",
        "\n",
        "*   Text chunking to maintain coherent information units\n",
        "\n",
        "*   Hypothetical Prompt Embedding Generation using an LLM to create multiple proxy questions per chunk\n",
        "*  Vector store creation using FAISS and Gemini embeddings\n",
        "\n",
        "\n",
        "*   Vector store creation using FAISS and OpenAI embeddings\n",
        "Retriever setup for querying the processed documents\n",
        "\n",
        "\n",
        "*   Evaluation of the RAG system\n"
      ],
      "metadata": {
        "id": "GIAAPNMhnCK5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "294muo2BmdfP",
        "outputId": "ff279cfe-51ae-4fd3-b8b0-5e61997b555a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: futures, faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0 futures-3.0.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "concurrent"
                ]
              },
              "id": "024fab9eb41546d1be48430502f3b820"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install faiss-cpu futures langchain-community tqdm langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "from dotenv import load_dotenv\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from typing import List\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "PATH = '/content/GYM.pdf'\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 50\n",
        "def generate_hypothetical_embeddings(chunk_text : str):\n",
        "  model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",google_api_key=GOOGLE_API_KEY,\n",
        "                             temperature=0,convert_system_message_to_human=True)\n",
        "  embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)\n",
        "  template = \"\"\"Analyze the input text and generate essential questions that, when answered, \\\n",
        "        capture the main points of the text. Each question should be one line, \\\n",
        "        without numbering or prefixes.\\n\\n \\\n",
        "        Text:\\n{chunk_text}\\n\\nQuestions:\\n \"\"\"\n",
        "  question_gen_prompt = ChatPromptTemplate.from_template(template)\n",
        "  question_chain = question_gen_prompt | model | StrOutputParser()\n",
        "  questions = question_chain.invoke({\"chunk_text\": chunk_text})\n",
        "  return chunk_text, embeddings.embed_documents(questions)\n",
        "\n",
        "def prepare_vectorstore(chunks : List[str]):\n",
        "  vector_store = None\n",
        "\n",
        "  with ThreadPoolExecutor() as pool:\n",
        "      futures = [pool.submit(generate_hypothetical_embeddings, c) for c in chunks]\n",
        "\n",
        "      # Process embeddings as they complete\n",
        "      for f in tqdm(as_completed(futures), total=len(chunks)):\n",
        "\n",
        "          chunk, vectors = f.result()  # Retrieve the processed chunk and its embeddings\n",
        "\n",
        "          if vector_store == None:\n",
        "              vector_store = FAISS(\n",
        "                  embedding_function=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY),  # Define embedding model\n",
        "                  index=faiss.IndexFlatL2(len(vectors[0])),  # Define an L2 index for similarity search\n",
        "                  docstore=InMemoryDocstore(),  # Use in-memory document storage\n",
        "                  index_to_docstore_id={}  # Maintain index-to-document mapping\n",
        "              )\n",
        "\n",
        "          # Pair the chunk's content with each generated embedding vector.\n",
        "          # Each chunk is inserted multiple times, once for each prompt vector\n",
        "          chunks_with_embedding_vectors = [(chunk.page_content, vec) for vec in vectors]\n",
        "\n",
        "          # Add embeddings to the store\n",
        "          vector_store.add_embeddings(chunks_with_embedding_vectors)\n",
        "\n",
        "  return vector_store  # Return the populated vector store\n",
        "\n",
        "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        path: The path to the PDF file.\n",
        "        chunk_size: The desired size of each text chunk.\n",
        "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded book content.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load PDF documents\n",
        "    loader = PyPDFLoader(path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    cleaned_texts = [\n",
        "    Document(page_content=chunk.page_content.replace('t', ' '), metadata=chunk.metadata)\n",
        "    for chunk in texts]\n",
        "    cleaned_texts = texts\n",
        "\n",
        "    vectorstore = prepare_vectorstore(cleaned_texts)\n",
        "\n",
        "    return vectorstore\n",
        "chunks_vector_store = encode_pdf(PATH, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "chunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "test_query = \"What are the exercises on push day\"\n",
        "context = chunks_query_retriever.get_relevant_documents(test_query)\n",
        "unique_context = []\n",
        "seen = set()\n",
        "\n",
        "for doc in context:\n",
        "    if doc.page_content not in seen:\n",
        "        seen.add(doc.page_content)\n",
        "        unique_context.append(doc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFCrcqThnwBA",
        "outputId": "40a065a8-bbbd-4a7d-a29b-580e331a2bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/langchain_google_genai/chat_models.py:424: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n",
            "100%|██████████| 3/3 [00:03<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(id='c2b7f076-bad3-4485-ab8b-5508e45dd48b', metadata={}, page_content='• Russian Twists – 3x30\\n• Mountain Climbers – 3x30 sec\\n6 Day 5 - Pull (Back + Arms Burnout)\\n• Lat Pulldown – 4x12\\n• Dumbbell Shrugs – 3x15\\n• Dumbbell Preacher Curl (if bench available) – 3x12\\n• Seated Rows or T-Bar Rows – 3x12\\n• Rope Hammer Curl (if cable available) – 3x15\\n7 Day 6 - Push (Chest & Arms Emphasis)\\n• Flat Dumbbell Press – 4x10\\n• Dumbbell Lateral Raise – 3x15\\n• Dumbbell Skull Crushers – 3x12\\n• Pushups – 3xAMRAP\\n• Chest Dips (if possible) or Incline Pushups – 3x10\\n8 Day 7 - Core & Abs\\n• Hanging Leg Raises – 3x15\\n• Bicycle Crunches – 3x30\\n• Decline Situps or Weighted Crunches – 3x15\\n• Ab Rollouts (if available) – 3x12\\n• Plank Variations – 3 sets\\n3')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(unique_context):\n",
        "    print(f\"\\n--- Document {i+1} ---\")\n",
        "    print(doc.page_content[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c4JRXv4vAeD",
        "outputId": "e467886b-a089-43bd-8cae-94eadc9985c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Document 1 ---\n",
            "• Russian Twists – 3x30\n",
            "• Mountain Climbers – 3x30 sec\n",
            "6 Day 5 - Pull (Back + Arms Burnout)\n",
            "• Lat Pulldown – 4x12\n",
            "• Dumbbell Shrugs – 3x15\n",
            "• Dumbbell Preacher Curl (if bench available) – 3x12\n",
            "• Seated Rows or T-Bar Rows – 3x12\n",
            "• Rope Hammer Curl (if cable available) – 3x15\n",
            "7 Day 6 - Push (Chest & Arms Emphasis)\n",
            "• Flat Dumbbell Press – 4x10\n",
            "• Dumbbell Lateral Raise – 3x15\n",
            "• Dumbbell Skull Crushers – 3x12\n",
            "• Pushups – 3xAMRAP\n",
            "• Chest Dips (if possible) or Incline Pushups – 3x10\n",
            "8 Day 7 - Core & Abs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_rag(retriever, questions_and_answers):\n",
        "    correct = 0\n",
        "    for question, expected_answer in questions_and_answers:\n",
        "        context = retriever.invoke(question)\n",
        "        combined_context = \" \".join([doc.page_content for doc in context])\n",
        "        print(f\"\\nQ: {question}\\nContext:\\n{combined_context}\\nExpected Answer: {expected_answer}\")\n",
        "evaluate_rag(chunks_query_retriever, [(\"What are the exercises on push day?\", \"Expected answer here\")])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zx4l6it19jU",
        "outputId": "05c16533-4341-4a34-c699-6b07fdd103ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What are the exercises on push day?\n",
            "Context:\n",
            "• Russian Twists – 3x30\n",
            "• Mountain Climbers – 3x30 sec\n",
            "6 Day 5 - Pull (Back + Arms Burnout)\n",
            "• Lat Pulldown – 4x12\n",
            "• Dumbbell Shrugs – 3x15\n",
            "• Dumbbell Preacher Curl (if bench available) – 3x12\n",
            "• Seated Rows or T-Bar Rows – 3x12\n",
            "• Rope Hammer Curl (if cable available) – 3x15\n",
            "7 Day 6 - Push (Chest & Arms Emphasis)\n",
            "• Flat Dumbbell Press – 4x10\n",
            "• Dumbbell Lateral Raise – 3x15\n",
            "• Dumbbell Skull Crushers – 3x12\n",
            "• Pushups – 3xAMRAP\n",
            "• Chest Dips (if possible) or Incline Pushups – 3x10\n",
            "8 Day 7 - Core & Abs\n",
            "• Hanging Leg Raises – 3x15\n",
            "• Bicycle Crunches – 3x30\n",
            "• Decline Situps or Weighted Crunches – 3x15\n",
            "• Ab Rollouts (if available) – 3x12\n",
            "• Plank Variations – 3 sets\n",
            "3 • Russian Twists – 3x30\n",
            "• Mountain Climbers – 3x30 sec\n",
            "6 Day 5 - Pull (Back + Arms Burnout)\n",
            "• Lat Pulldown – 4x12\n",
            "• Dumbbell Shrugs – 3x15\n",
            "• Dumbbell Preacher Curl (if bench available) – 3x12\n",
            "• Seated Rows or T-Bar Rows – 3x12\n",
            "• Rope Hammer Curl (if cable available) – 3x15\n",
            "7 Day 6 - Push (Chest & Arms Emphasis)\n",
            "• Flat Dumbbell Press – 4x10\n",
            "• Dumbbell Lateral Raise – 3x15\n",
            "• Dumbbell Skull Crushers – 3x12\n",
            "• Pushups – 3xAMRAP\n",
            "• Chest Dips (if possible) or Incline Pushups – 3x10\n",
            "8 Day 7 - Core & Abs\n",
            "• Hanging Leg Raises – 3x15\n",
            "• Bicycle Crunches – 3x30\n",
            "• Decline Situps or Weighted Crunches – 3x15\n",
            "• Ab Rollouts (if available) – 3x12\n",
            "• Plank Variations – 3 sets\n",
            "3 • Russian Twists – 3x30\n",
            "• Mountain Climbers – 3x30 sec\n",
            "6 Day 5 - Pull (Back + Arms Burnout)\n",
            "• Lat Pulldown – 4x12\n",
            "• Dumbbell Shrugs – 3x15\n",
            "• Dumbbell Preacher Curl (if bench available) – 3x12\n",
            "• Seated Rows or T-Bar Rows – 3x12\n",
            "• Rope Hammer Curl (if cable available) – 3x15\n",
            "7 Day 6 - Push (Chest & Arms Emphasis)\n",
            "• Flat Dumbbell Press – 4x10\n",
            "• Dumbbell Lateral Raise – 3x15\n",
            "• Dumbbell Skull Crushers – 3x12\n",
            "• Pushups – 3xAMRAP\n",
            "• Chest Dips (if possible) or Incline Pushups – 3x10\n",
            "8 Day 7 - Core & Abs\n",
            "• Hanging Leg Raises – 3x15\n",
            "• Bicycle Crunches – 3x30\n",
            "• Decline Situps or Weighted Crunches – 3x15\n",
            "• Ab Rollouts (if available) – 3x12\n",
            "• Plank Variations – 3 sets\n",
            "3\n",
            "Expected Answer: Expected answer here\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adaptive RAG:\n",
        "\n",
        "This RAG model is modified in its process of retrieving data from the document corpus. Instead of passing the query verbatim and retrieving data according to it we classify our queries into 4 categories and according to these categories run the retrieval process:\n",
        "\n",
        "*   Factual : For these types of queries,the query is enhanced using an LLM and used to retrieve documents using this more precise query.\n",
        "\n",
        "*   Analytical: For these types of queries, LLM create sub topics and retrieve documents for all these sub topics and provide it which helps the LLM generate a more detailed and in depth response\n",
        "*   Opinion: LLMs are used to identify different view points for the query and retrieve documents for each view point allowing the LLM to provide a complete and diverse response.\n",
        "\n",
        "\n",
        "*   Contextual: Incorporates context provided by the user and uses this contextual query to retrieve data based on it.\n",
        "\n"
      ],
      "metadata": {
        "id": "2hn9-Njn-h-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from typing import Dict, Any,List\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9XN8N2LBkSW",
        "outputId": "a659bf96-4bf3-49a7-cbe0-44841cda2d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class categories_options(BaseModel):\n",
        "        category: str = Field(description=\"The category of the query, the options are: Factual, Analytical, Opinion, or Contextual\", example=\"Factual\")\n",
        "\n",
        "\n",
        "class QueryClassifier:\n",
        "    def __init__(self):\n",
        "        self.llm = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-1.5-flash\", max_tokens=4000, google_api_key=GOOGLE_API_KEY)\n",
        "        self.prompt = PromptTemplate(\n",
        "            input_variables=[\"query\"],\n",
        "            template=\"Classify the following query into one of these categories: Factual, Analytical, Opinion, or Contextual.\\nQuery: {query}\\nCategory:\"\n",
        "        )\n",
        "        self.chain = self.prompt | self.llm.with_structured_output(categories_options)\n",
        "\n",
        "\n",
        "    def classify(self, query):\n",
        "        print(\"clasiffying query\")\n",
        "        return self.chain.invoke(query).category\n",
        "class BaseRetrievalStrategy:\n",
        "    def __init__(self, texts):\n",
        "        self.embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=0)\n",
        "        self.documents = text_splitter.create_documents(texts)\n",
        "        self.db = FAISS.from_documents(self.documents, self.embeddings)\n",
        "        self.llm = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-1.5-flash\", max_tokens=4000, google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "    def retrieve(self, query, k=4):\n",
        "        return self.db.similarity_search(query, k=k)\n",
        "class relevant_score(BaseModel):\n",
        "        score: float = Field(description=\"The relevance score of the document to the query\", example=8.0)\n",
        "\n",
        "class FactualRetrievalStrategy(BaseRetrievalStrategy):\n",
        "    def retrieve(self, query, k=4):\n",
        "        print(\"retrieving factual\")\n",
        "        # Use LLM to enhance the query\n",
        "        enhanced_query_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\"],\n",
        "            template=\"Enhance this factual query for better information retrieval: {query}\"\n",
        "        )\n",
        "        query_chain = enhanced_query_prompt | self.llm\n",
        "        enhanced_query = query_chain.invoke(query).content\n",
        "        print(f'enhanced query: {enhanced_query}')\n",
        "\n",
        "        # Retrieve documents using the enhanced query\n",
        "        docs = self.db.similarity_search(enhanced_query, k=k*2)\n",
        "\n",
        "        # Use LLM to rank the relevance of retrieved documents\n",
        "        ranking_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"doc\"],\n",
        "            template=\"On a scale of 1-10, how relevant is this document to the query: '{query}'?\\nDocument: {doc}\\nRelevance score:\"\n",
        "        )\n",
        "        ranking_chain = ranking_prompt | self.llm.with_structured_output(relevant_score)\n",
        "\n",
        "        ranked_docs = []\n",
        "        print(\"ranking docs\")\n",
        "        for doc in docs:\n",
        "            input_data = {\"query\": enhanced_query, \"doc\": doc.page_content}\n",
        "            score = float(ranking_chain.invoke(input_data).score)\n",
        "            ranked_docs.append((doc, score))\n",
        "\n",
        "        # Sort by relevance score and return top k\n",
        "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "        return [doc for doc, _ in ranked_docs[:k]]\n",
        "class SelectedIndices(BaseModel):\n",
        "    indices: List[int] = Field(description=\"Indices of selected documents\", example=[0, 1, 2, 3])\n",
        "\n",
        "class SubQueries(BaseModel):\n",
        "    sub_queries: List[str] = Field(description=\"List of sub-queries for comprehensive analysis\", example=[\"What is the population of New York?\", \"What is the GDP of New York?\"])\n",
        "\n",
        "class AnalyticalRetrievalStrategy(BaseRetrievalStrategy):\n",
        "    def retrieve(self, query, k=4):\n",
        "        print(\"retrieving analytical\")\n",
        "        # Use LLM to generate sub-queries for comprehensive analysis\n",
        "        sub_queries_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"k\"],\n",
        "            template=\"Generate {k} sub-questions for: {query}\"\n",
        "        )\n",
        "\n",
        "        llm = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-1.5-flash\", max_tokens=4000, google_api_key=GOOGLE_API_KEY)\n",
        "        sub_queries_chain = sub_queries_prompt | llm.with_structured_output(SubQueries)\n",
        "\n",
        "        input_data = {\"query\": query, \"k\": k}\n",
        "        sub_queries = sub_queries_chain.invoke(input_data).sub_queries\n",
        "        print(f'sub queries for comprehensive analysis: {sub_queries}')\n",
        "\n",
        "        all_docs = []\n",
        "        for sub_query in sub_queries:\n",
        "            all_docs.extend(self.db.similarity_search(sub_query, k=2))\n",
        "\n",
        "        # Use LLM to ensure diversity and relevance\n",
        "        diversity_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"docs\", \"k\"],\n",
        "            template=\"\"\"Select the most diverse and relevant set of {k} documents for the query: '{query}'\\nDocuments: {docs}\\n\n",
        "            Return only the indices of selected documents as a list of integers.\"\"\"\n",
        "        )\n",
        "        diversity_chain = diversity_prompt | self.llm.with_structured_output(SelectedIndices)\n",
        "        docs_text = \"\\n\".join([f\"{i}: {doc.page_content[:50]}...\" for i, doc in enumerate(all_docs)])\n",
        "        input_data = {\"query\": query, \"docs\": docs_text, \"k\": k}\n",
        "        selected_indices_result = diversity_chain.invoke(input_data).indices\n",
        "        print(f'selected diverse and relevant documents')\n",
        "\n",
        "        return [all_docs[i] for i in selected_indices_result if i < len(all_docs)]\n",
        "class OpinionRetrievalStrategy(BaseRetrievalStrategy):\n",
        "    def retrieve(self, query, k=3):\n",
        "        print(\"retrieving opinion\")\n",
        "        # Use LLM to identify potential viewpoints\n",
        "        viewpoints_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"k\"],\n",
        "            template=\"Identify {k} distinct viewpoints or perspectives on the topic: {query}\"\n",
        "        )\n",
        "        viewpoints_chain = viewpoints_prompt | self.llm\n",
        "        input_data = {\"query\": query, \"k\": k}\n",
        "        viewpoints = viewpoints_chain.invoke(input_data).content.split('\\n')\n",
        "        print(f'viewpoints: {viewpoints}')\n",
        "\n",
        "        all_docs = []\n",
        "        for viewpoint in viewpoints:\n",
        "            all_docs.extend(self.db.similarity_search(f\"{query} {viewpoint}\", k=2))\n",
        "\n",
        "        # Use LLM to classify and select diverse opinions\n",
        "        opinion_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"docs\", \"k\"],\n",
        "            template=\"Classify these documents into distinct opinions on '{query}' and select the {k} most representative and diverse viewpoints:\\nDocuments: {docs}\\nSelected indices:\"\n",
        "        )\n",
        "        opinion_chain = opinion_prompt | self.llm.with_structured_output(SelectedIndices)\n",
        "\n",
        "        docs_text = \"\\n\".join([f\"{i}: {doc.page_content[:100]}...\" for i, doc in enumerate(all_docs)])\n",
        "        input_data = {\"query\": query, \"docs\": docs_text, \"k\": k}\n",
        "        selected_indices = opinion_chain.invoke(input_data).indices\n",
        "        print(f'selected diverse and relevant documents')\n",
        "\n",
        "        return [all_docs[int(i)] for i in selected_indices.split() if i.isdigit() and int(i) < len(all_docs)]\n",
        "class ContextualRetrievalStrategy(BaseRetrievalStrategy):\n",
        "    def retrieve(self, query, k=4, user_context=None):\n",
        "        print(\"retrieving contextual\")\n",
        "        # Use LLM to incorporate user context into the query\n",
        "        context_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"context\"],\n",
        "            template=\"Given the user context: {context}\\nReformulate the query to best address the user's needs: {query}\"\n",
        "        )\n",
        "        context_chain = context_prompt | self.llm\n",
        "        input_data = {\"query\": query, \"context\": user_context or \"No specific context provided\"}\n",
        "        contextualized_query = context_chain.invoke(input_data).content\n",
        "        print(f'contextualized query: {contextualized_query}')\n",
        "\n",
        "        # Retrieve documents using the contextualized query\n",
        "        docs = self.db.similarity_search(contextualized_query, k=k*2)\n",
        "\n",
        "        # Use LLM to rank the relevance of retrieved documents considering the user context\n",
        "        ranking_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"context\", \"doc\"],\n",
        "            template=\"Given the query: '{query}' and user context: '{context}', rate the relevance of this document on a scale of 1-10:\\nDocument: {doc}\\nRelevance score:\"\n",
        "        )\n",
        "        ranking_chain = ranking_prompt | self.llm.with_structured_output(relevant_score)\n",
        "        print(\"ranking docs\")\n",
        "\n",
        "        ranked_docs = []\n",
        "        for doc in docs:\n",
        "            input_data = {\"query\": contextualized_query, \"context\": user_context or \"No specific context provided\", \"doc\": doc.page_content}\n",
        "            score = float(ranking_chain.invoke(input_data).score)\n",
        "            ranked_docs.append((doc, score))\n",
        "\n",
        "\n",
        "        # Sort by relevance score and return top k\n",
        "        ranked_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return [doc for doc, _ in ranked_docs[:k]]\n",
        "class AdaptiveRetriever:\n",
        "    def __init__(self, texts: List[str]):\n",
        "        self.classifier = QueryClassifier()\n",
        "        self.strategies = {\n",
        "            \"Factual\": FactualRetrievalStrategy(texts),\n",
        "            \"Analytical\": AnalyticalRetrievalStrategy(texts),\n",
        "            \"Opinion\": OpinionRetrievalStrategy(texts),\n",
        "            \"Contextual\": ContextualRetrievalStrategy(texts)\n",
        "        }\n",
        "\n",
        "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        category = self.classifier.classify(query)\n",
        "        strategy = self.strategies[category]\n",
        "        return strategy.retrieve(query)\n",
        "class PydanticAdaptiveRetriever(BaseRetriever):\n",
        "    adaptive_retriever: AdaptiveRetriever = Field(exclude=True)\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        return self.adaptive_retriever.get_relevant_documents(query)\n",
        "\n",
        "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
        "        return self.get_relevant_documents(query)\n",
        "class AdaptiveRAG:\n",
        "    def __init__(self, texts: List[str]):\n",
        "        adaptive_retriever = AdaptiveRetriever(texts)\n",
        "        self.retriever = PydanticAdaptiveRetriever(adaptive_retriever=adaptive_retriever)\n",
        "        self.llm = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-1.5-flash\", max_tokens=4000, google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "        # Create a custom prompt\n",
        "        prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "        {context}\n",
        "\n",
        "        Question: {question}\n",
        "        Answer:\"\"\"\n",
        "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "        # Create the LLM chain\n",
        "        self.llm_chain = prompt | self.llm\n",
        "\n",
        "\n",
        "\n",
        "    def answer(self, query: str) -> str:\n",
        "        docs = self.retriever.get_relevant_documents(query)\n",
        "        input_data = {\"context\": \"\\n\".join([doc.page_content for doc in docs]), \"question\": query}\n",
        "        return self.llm_chain.invoke(input_data)\n",
        "texts = [\n",
        "    \"The Earth orbits the Sun at an average distance of about 149.6 million kilometers (1 AU). Its orbit is nearly circular, taking about 365.25 days to complete, defining a year. The Earth's axis is tilted at 23.5°, causing seasons as sunlight distribution changes. When the Northern Hemisphere tilts toward the Sun, it’s summer there and winter in the Southern Hemisphere. Equinoxes (March 21, Sept 23) have equal day and night; solstices (June 21, Dec 21) mark extremes. Earth rotates in about 24 hours, creating day and night as the Sun appears to rise in the east. Solar energy from the Sun drives Earth’s weather, photosynthesis, and ecosystems. The Sun’s light is filtered by Earth’s atmosphere, which also protects against harmful radiation. Solar activity like flares and CMEs can affect satellites and power grids. Earth’s magnetic field shields us from solar winds, contributing to auroras. The Sun’s gravity holds Earth in orbit, while Earth exerts a small pull in return. Earth is the third planet in the solar system and lies in the habitable zone. The Sun is a G-type main-sequence star, about 4.6 billion years old, made mostly of hydrogen and helium. Aphelion (farthest point) is ~152.1 million km in July; perihelion (closest) is ~147.1 million km in January. Seasons depend more on axial tilt than distance. The Sun's energy varies by latitude due to Earth's curvature. Sunspots are cooler, dark solar regions that follow an 11-year cycle. Solar eclipses occur when the Moon blocks the Sun, made possible by their apparent similar sizes. The Sun’s radiation sustains life and drives the climate. Earth’s orbital dynamics and tilt shape day length and weather. The Earth-Sun interaction influences everything from agriculture to calendars. Ancient cultures used solar movement for timekeeping and alignment of structures. Without the Sun’s gravity and energy, Earth would be lifeless and drifting. Understanding this relationship is vital for climate studies and space missions. It is this unique balance that makes Earth habitable.\"\n",
        "    ]\n",
        "rag_system = AdaptiveRAG(texts)\n",
        "\n",
        "factual_result = rag_system.answer(\"What is the distance between the Earth and the Sun?\").content\n",
        "print(f\"Answer: {factual_result}\")\n",
        "\n",
        "analytical_result = rag_system.answer(\"How does the Earth's distance from the Sun affect its climate?\").content\n",
        "print(f\"Answer: {analytical_result}\")\n",
        "\n",
        "opinion_result = rag_system.answer(\"What are the different theories about the origin of life on Earth?\").content\n",
        "print(f\"Answer: {opinion_result}\")\n",
        "\n",
        "contextual_result = rag_system.answer(\"How does the Earth's position in the Solar System influence its habitability?\").content\n",
        "print(f\"Answer: {contextual_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gywLSLH2_phY",
        "outputId": "419ac211-cb3b-4f23-a09b-e566e82bcf9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-1941944917>:181: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
            "  class PydanticAdaptiveRetriever(BaseRetriever):\n",
            "<ipython-input-3-1941944917>:181: DeprecationWarning: Retrievers must implement abstract `_aget_relevant_documents` method instead of `aget_relevant_documents`\n",
            "  class PydanticAdaptiveRetriever(BaseRetriever):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clasiffying query\n",
            "retrieving factual\n",
            "enhanced query: Several enhancements are possible, depending on the desired level of precision and context:\n",
            "\n",
            "**Option 1 (More precise):**\n",
            "\n",
            "> What is the average distance between the Earth and the Sun (in astronomical units and kilometers)?  Specify whether this refers to the semi-major axis of Earth's orbit or another measure (e.g., perihelion, aphelion).\n",
            "\n",
            "This clarifies the desired units and accounts for the elliptical nature of Earth's orbit, requesting specific terms for different distances.\n",
            "\n",
            "**Option 2 (Contextual):**\n",
            "\n",
            "> What is the distance between the Earth and the Sun at [specific date or time]?  Specify the units used (e.g., kilometers, astronomical units).\n",
            "\n",
            "This adds a temporal element, making the query relevant to a specific point in time.\n",
            "\n",
            "**Option 3 (Most precise and comprehensive):**\n",
            "\n",
            "> What is the current distance between the Earth and the Sun, expressed in both astronomical units and kilometers?  Provide the values for perihelion, aphelion, and the current semi-major axis.  Include a source for the data.\n",
            "\n",
            "This requests multiple data points and emphasizes the need for a reliable source.\n",
            "\n",
            "\n",
            "The best option depends on the user's needs.  The original query is too simplistic for accurate scientific information retrieval.  Adding detail and specifying units and context significantly improves the results.\n",
            "ranking docs\n",
            "Answer: The average distance between the Earth and the Sun is about 149.6 million kilometers (1 AU).  The distance varies throughout the year, with the farthest point (aphelion) being approximately 152.1 million km and the closest point (perihelion) being approximately 147.1 million km.\n",
            "clasiffying query\n",
            "retrieving analytical\n",
            "sub queries for comprehensive analysis: [\"How does the intensity of solar radiation change with Earth's distance from the Sun?\", \"What is the relationship between Earth's orbital eccentricity and its distance from the Sun?\", \"How do variations in Earth's distance from the Sun affect the length of seasons?\", \"What is the role of Earth's axial tilt in modifying the climatic effects of its distance from the Sun?\"]\n",
            "selected diverse and relevant documents\n",
            "Answer: The provided text states that seasons depend more on the Earth's axial tilt than its distance from the Sun.  While the Earth is closer to the Sun in January (perihelion) and farther in July (aphelion), this difference in distance has less impact on the Earth's climate than the tilt of its axis.\n",
            "clasiffying query\n",
            "retrieving contextual\n",
            "contextualized query: What are the major scientific hypotheses regarding abiogenesis (the origin of life on Earth)?\n",
            "ranking docs\n",
            "Answer: The provided text describes the Earth-Sun relationship and doesn't offer any information about theories on the origin of life on Earth.  Therefore, I don't know.\n",
            "clasiffying query\n",
            "retrieving analytical\n",
            "sub queries for comprehensive analysis: [\"How does Earth's distance from the Sun affect its temperature range and the presence of liquid water?\", \"What is the role of Earth's axial tilt in creating seasons and influencing climate patterns?\", \"How does Earth's gravitational interaction with other planets and the Sun affect its orbit and stability?\", \"What is the impact of Earth's location within the Goldilocks zone on its habitability?\"]\n",
            "selected diverse and relevant documents\n",
            "Answer: Earth's position in the habitable zone, as the third planet from the Sun, allows it to receive the right amount of solar energy to support liquid water and life.  The Sun's gravity also keeps Earth in a stable orbit.\n"
          ]
        }
      ]
    }
  ]
}