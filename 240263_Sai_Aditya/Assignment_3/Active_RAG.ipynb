{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This RAG implementation is based on a simple observation: users often ask vague or underspecified questions. Instead of blindly trying to answer such queries, this system first checks whether the input is clear enough to proceed. If the query is ambiguous, it passes it through an LLM which generates a clarifying follow-up prompt - something that gets sent back to the user asking exactly what they meant. Only after the system has enough context does it move forward.\n",
        "\n",
        "If the query is already clear, it is passed into a function called reformulate_query, which makes the question more specific and direct - improving its quality for downstream retrieval. This refined version is then used to fetch relevant context from the vector store and generate a final answer using the LLM.\n",
        "\n",
        "In this implementation, I have used a government leave rules document that I found online.\n",
        "\n",
        "This kind of setup helps prevent common problems like hallucination or irrelevant answers, which usually happen when the model has to “guess” what the user meant. By introducing this clarification and reformulation loop, the system ensures that the prompt reaching the answering model is as precise and context-aware as possible. It leads to more structured, relevant, and accurate answers.\n",
        "\n",
        "This is also how most advanced LLM-based systems like ChatGPT and Perplexity handle vague queries - not just to improve answer quality, but to reduce unnecessary computation spent trying to figure out what the user was actually asking."
      ],
      "metadata": {
        "id": "VPaHynOj4uMD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uyuHpCP3qr5A"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain chromadb cohere pypdf groq langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import CohereEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "84qew5MWtL4b"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "cohere_api_key = userdata.get('COHERE_API_KEY')\n",
        "groq_api_key = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "1vNHIUJztcH5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"/content/LeaveRulesRevised.pdf\")\n",
        "docs = loader.load()\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "chunks = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "eM8Gyrslth34"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = CohereEmbeddings(model=\"embed-english-v3.0\",cohere_api_key=cohere_api_key,user_agent='xyz')\n",
        "texts = [doc.page_content for doc in chunks]\n",
        "metadatas = [doc.metadata for doc in chunks]\n",
        "db = Chroma.from_texts(texts=texts, embedding=embeddings, metadatas=metadatas, persist_directory=\"./leaverules_db\")\n",
        "\n",
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "7WHUdt7It2AH"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q groq langchain_groq"
      ],
      "metadata": {
        "id": "dVqzG-dzubDF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "groq_api_key = userdata.get('GROQ_API_KEY')\n",
        "llm = ChatGroq(groq_api_key=groq_api_key,\n",
        "    model_name=\"llama3-8b-8192\")"
      ],
      "metadata": {
        "id": "7ZSCXaoWuXgl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_query_ambiguous(query: str) -> bool:\n",
        "    prompt = f\"\"\"You are a helpful assistant. A user asked: \"{query}\".\n",
        "Is this query vague or ambiguous in a way that would make it hard to answer without asking them a follow-up question? Reply with only 'Yes' or 'No'.\"\"\"\n",
        "    res = llm.invoke(prompt)\n",
        "    return \"yes\" in res.content.lower()"
      ],
      "metadata": {
        "id": "iCMuaFz1uihF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_for_clarification(query: str) -> str:\n",
        "    prompt = f\"\"\"The user query is unclear or ambiguous: '{query}'. Ask a clarifying question to get missing information.\"\"\"\n",
        "    return llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "o_l9vW-iu2Dm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clarification_prompt(query: str) -> str:\n",
        "    prompt = f\"\"\"The user said: \"{query}\".\n",
        "Ask a polite follow-up question to clarify their intent or provide missing details. Keep it short and direct.\"\"\"\n",
        "    return llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "2q2WbdvFxaww"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reformulate_query(query: str) -> str:\n",
        "    prompt = f\"\"\"Take the user's query: \"{query}\" and rewrite it to make it clear, specific, and helpful for retrieving information from a document about government leave rules.\"\"\"\n",
        "    return llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "FuGnWQM-zlgu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def active_rag_loop(user_query: str):\n",
        "\n",
        "    if is_query_ambiguous(user_query):                                       #asking for clarification in case the prompt is unclear\n",
        "        clarification = get_clarification_prompt(user_query)\n",
        "        return {\n",
        "            \"action\": \"clarify_needed\",\n",
        "            \"message\": f\"Clarification needed.\\n{clarification}\"\n",
        "        }\n",
        "\n",
        "    refined_query = reformulate_query(user_query)\n",
        "\n",
        "    docs = retriever.invoke(refined_query)\n",
        "\n",
        "    if docs == []:\n",
        "        return {\n",
        "            \"action\": \"no_results\",\n",
        "            \"message\": \" Couldn't find anything relevant. Try rephrasing or giving more context.\"\n",
        "        }\n",
        "\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
        "    answer = qa_chain.run(refined_query)\n",
        "\n",
        "    return {\n",
        "        \"action\": \"answer\",\n",
        "        \"message\": answer\n",
        "    }"
      ],
      "metadata": {
        "id": "b6cLFflnz_Gb"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"tell me about leave\"\n",
        "\n",
        "response = active_rag_loop(query)\n",
        "\n",
        "print(\"System:\", response[\"message\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZoTQ7vr0hyJ",
        "outputId": "260d47dc-f34c-403b-be43-6fbfe141b21b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Clarification needed.\n",
            "content='\"Leave\" can refer to many things, such as vacation time, sick leave, or quitting a job. Can you please specify what type of \"leave\" you are referring to?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 42, 'total_tokens': 80, 'completion_time': 0.153826892, 'prompt_time': 0.058908177, 'queue_time': 0.849473121, 'total_time': 0.212735069}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_24ec19897b', 'finish_reason': 'stop', 'logprobs': None} id='run--16de0a0c-403a-42a8-a0ab-608c8efa6a4c-0' usage_metadata={'input_tokens': 42, 'output_tokens': 38, 'total_tokens': 80}\n"
          ]
        }
      ]
    }
  ]
}