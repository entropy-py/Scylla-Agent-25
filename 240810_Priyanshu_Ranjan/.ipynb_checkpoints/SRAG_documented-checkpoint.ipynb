{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9450eb",
   "metadata": {},
   "source": [
    "# ðŸ“„ Self-RAG Notebook Documentation\n",
    "This notebook sets up a **Self-RAG (Self-Reflective Retrieval-Augmented Generation)** pipeline using `llama-index`.\n",
    "\n",
    "## Structure\n",
    "1. **Language Model & Embedding Setup** - Initializes the LLM and embedding model.\n",
    "2. **Data Loading** - Loads documents for indexing.\n",
    "3. *(Later Sections Expected)* - Retrieval decisions, query engine setup, agent workflows.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28805faf-5fc8-49b7-9c1a-bba5f4a66bcc",
   "metadata": {},
   "source": [
    "## Setup Language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "055aab5e-6144-49a1-afc6-e8b4be0a2162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from a .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Call to load environment variables into the environment\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8397427c-d0e6-4f51-a10b-4b8dac08a4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itspriiyanshu/Desktop/Scylla-Agent-25/240810_Priyanshu_Ranjan/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.groq import Groq\n",
    "# Initialize a Groq LLM (LLaMA3 70B) for natural language processing\n",
    "llm = Groq(model=\"llama3-70b-8192\")\n",
    "# Load a local embedding model (nomic-embed-text) using Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "# Load a local embedding model (nomic-embed-text) using Ollama\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab721f20-2f4a-4c60-9bda-db9d484d00c9",
   "metadata": {},
   "source": [
    "## Load, Chunk and Embed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beaaec0c-825e-4af6-a5f7-c3e53f6e7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and load documents from the local './data' directory\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Read and load documents from the local './data' directory\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78cd72e0-c94b-4626-94d7-a8c88ba60645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "parser = SentenceSplitter(chunk_size=256, chunk_overlap=16)\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6e98d00-6ad3-46d4-8866-a9fff7b10b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "# index.storage_context.persist(persist_dir=\"./storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a327b-3935-4071-a833-20b71296fe89",
   "metadata": {},
   "source": [
    "## Create Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dc825d6-9678-4808-8a23-fe116e9351be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    ")\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from typing import List\n",
    "from llama_index.core.schema import TextNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81f36ef5-d287-4304-9195-c6c561e2de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoRetrieval(Event):\n",
    "    query: str\n",
    "\n",
    "class RetrieveEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class RelevanceEval(Event):\n",
    "    query: str\n",
    "    retrieved_nodes: List[TextNode]\n",
    "\n",
    "class WithRetrieval(Event):\n",
    "    query: str\n",
    "    relevant_context: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99472243-04d4-4561-8414-9f990a65d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sRAG(Workflow):\n",
    "    @step\n",
    "    async def decide_retrieval(self, ev: StartEvent) -> RetrieveEvent | NoRetrieval:\n",
    "        query = ev.query\n",
    "\n",
    "        prompt = f\"Given the query: '{query}', determine if retrieval is necessary. Output only 'Yes' or 'No'.\"\n",
    "        response = await llm.acomplete(prompt)\n",
    "        if str(response)==\"Yes\":\n",
    "            return RetrieveEvent(query=query)\n",
    "        else:\n",
    "            return NoRetrieval(query=query)\n",
    "    @step\n",
    "    async def retrieve(self, ev: RetrieveEvent) -> RelevanceEval:\n",
    "        query = ev.query\n",
    "        # storage_context = StorageContext.from_defaults(persist_dir=\"./storage\", embed_model=embed_model)\n",
    "        # index = load_index_from_storage(storage_context)\n",
    "        retriever = VectorIndexRetriever(index=index, similarity_top_k=3)\n",
    "        retrieved_nodes = retriever.retrieve(query)\n",
    "        text_nodes = [n.node for n in retrieved_nodes] #try sending with scores as well and make llm leverage them evaluate relevange\n",
    "        return RelevanceEval(query=query, retrieved_nodes=text_nodes)\n",
    "    @step\n",
    "    async def eval_relevance(self, ev: RelevanceEval) -> WithRetrieval | NoRetrieval:\n",
    "        retrieved_nodes = ev.retrieved_nodes\n",
    "        query = ev.query\n",
    "        relevant_context = []\n",
    "        for node in retrieved_nodes:\n",
    "            context = node.get_content()\n",
    "            prompt = f\"Given the query: '{query}' and the context: '{context}', determine if the context is relevant. Output only 'Relevant' or 'Irrelevant'.\"\n",
    "            response = await llm.acomplete(prompt)\n",
    "            if str(response)==\"Relevant\":\n",
    "                relevant_context.append(context)\n",
    "        if not relevant_context:\n",
    "            return NoRetrieval(query=query)\n",
    "        else:\n",
    "            return WithRetrieval(query=query, relevant_context=relevant_context)\n",
    "    @step\n",
    "    async def generate_with_context(self, ev: WithRetrieval) -> StopEvent:\n",
    "        query= ev.query\n",
    "        relevant_context = ev.relevant_context\n",
    "        context = \"\\n\".join(f\"- {doc}\" for doc in relevant_context)\n",
    "        # this has a flaw, joining contexts may lead to redundant data and possible chunk overlaps\n",
    "        # instead of joining contexts, we can try generating response over individual contexts \n",
    "        prompt = f\"Given the query '{query}' and the context '{context}', generate a response.\"\n",
    "        response = await llm.acomplete(prompt)\n",
    "        # print(str(response))\n",
    "        return StopEvent(result = str(response))\n",
    "    @step\n",
    "    async def generate_without_context(self, ev: NoRetrieval) -> StopEvent:\n",
    "        query= ev.query\n",
    "        prompt = f\"Given the query '{query}', generate a response.\"\n",
    "        response = await llm.acomplete(prompt)\n",
    "        # print(str(response))\n",
    "        return StopEvent(result = str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09955da3-a8e4-44e5-94ac-db932f73be5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srag.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"400\"\n",
       "            src=\"srag.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x78bd0dece650>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.utils.workflow import (\n",
    "    draw_all_possible_flows,\n",
    "    draw_most_recent_execution,\n",
    ")\n",
    "from IPython.display import IFrame\n",
    "\n",
    "\n",
    "# Draw all\n",
    "draw_all_possible_flows(sRAG, filename=\"srag.html\")\n",
    "IFrame(\"srag.html\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dcf42f-ba8e-4d4a-9cac-b39a1b2b2ed4",
   "metadata": {},
   "source": [
    "## Running the Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d589cd-50fb-48af-aa55-68afc14c5cd7",
   "metadata": {},
   "source": [
    "Example where **No Retrieval** needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "046689de-e0bc-41e7-88db-cc0d2b11d298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step decide_retrieval\n",
      "Step decide_retrieval produced event NoRetrieval\n",
      "Running step generate_without_context\n",
      "Step generate_without_context produced event StopEvent\n",
      "Here's a joke for you:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you laugh!\n"
     ]
    }
   ],
   "source": [
    "w = sRAG(timeout=120, verbose = True)\n",
    "result = await w.run(query=\"Write a joke\")\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae95b1f5-7833-4648-8056-3da02dba7633",
   "metadata": {},
   "source": [
    "Example where **Retrieval** needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dee491d0-35c0-42cc-bbe7-2ef29251aa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step decide_retrieval\n",
      "Step decide_retrieval produced event RetrieveEvent\n",
      "Running step retrieve\n",
      "Step retrieve produced event RelevanceEval\n",
      "Running step eval_relevance\n",
      "Step eval_relevance produced event WithRetrieval\n",
      "Running step generate_with_context\n",
      "Step generate_with_context produced event StopEvent\n",
      "Here is a response to the query \"Review for EE200A course\":\n",
      "\n",
      "The EE200A course, also known as Signals, Systems & Networks, is a fundamental course in the Electrical Engineering (EE) curriculum. According to the review, this course is the first true EE course that students will encounter on campus, and it deals with signals and their representations, systems, Fourier representations, and networks.\n",
      "\n",
      "The instructor for this course is reportedly one of the best on campus, which is a positive aspect. However, the course has some drawbacks. Notes are not provided, and students have to write notes in class, which can be challenging. Additionally, the assignments are lengthy, and the questions asked in the tests are often different from what is expected.\n",
      "\n",
      "Past papers are a great way to practice, but solutions are hard to find, making it difficult for students to know if their answers are correct or not. Overall, the difficulty level of this course is rated as 4 out of 5.\n",
      "\n",
      "In summary, EE200A is a crucial course in the EE curriculum, and while it has some challenges, it is essential to have a good instructor to guide students through the course.\n"
     ]
    }
   ],
   "source": [
    "w = sRAG(timeout=120, verbose = True)\n",
    "result = await w.run(query=\"Review for EE200A course\")\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d755acb",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "The notebook sets up the foundation for a Self-RAG pipeline. Additional cells would likely include:\n",
    "- Setting up decision logic for whether to retrieve or not\n",
    "- Query execution with and without retrieval\n",
    "- Multi-agent workflows\n",
    "\n",
    "Make sure your `.env` file and model dependencies (Groq API, Ollama) are properly configured before execution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
